---
title: "Learning a Visually Grounded Memory Assistant"
collection: publications
permalink: /publication/test
excerpt: 'We present WHERE ARE YOU? (WAY), a dataset of ∼6k dialogs in which two humans – an Observer and a Locator – complete a cooperative localization task.'
date: 2020-12-01
venue: 'Empirical Methods in Natural Language Processing.'
paperurl: 'https://www.aclweb.org/anthology/2020.emnlp-main.59.pdf'
website: 'https://meerahahn.github.io/way'
data: 'https://meerahahn.github.io/way/data'
code: 'https://github.com/meera1hahn/Graph_LED'
leaderboard: 'https://eval.ai/web/challenges/challenge-page/1206'
citation: 'Hahn, Meera, Jacob Krantz, Dhruv Batra, Devi Parikh, James Rehg, Stefan Lee, and Peter Anderson. "Where Are You? Localization from Embodied Dialog." In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 806-822. 2020.'
---

<!-- ---
title: "Learning a Visually Grounded Memory Assistant"
collection: publications
permalink: /publication/visualassistant
excerpt: 'We introduce a novel interface for large scale collection of human memory and assistance.Using the interface we collect the `The Visually Grounded Memory Assistant Dataset' which is aimed at developing our understanding of (1) the information people encode during navigation of 3D environments and (2) conditions under which people ask for memory assistance.'
date: 2020-01-01
venue: 'Arxiv.'
paperurl: 'http://meerahahn.github.io/files/visual_assistant.pdf'

citation: 'Meera Hahn, Kevin Carlberg, Ruta Desai, and James Hillis. "Learning a Visually Grounded Memory Assistant." arXiv (2020).'
--- -->